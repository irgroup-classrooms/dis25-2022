{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0413de38-f565-4809-bbe6-df7b485b7cfb",
   "metadata": {},
   "source": [
    "# Tutorial 4: Vector Semantics - Gensim, SpaCy, Word2Vec and GloVe\n",
    "\n",
    "After you have dealt with synsemantics in the last tutorial, this tutorial is dedicated to vector semantics. Instead of manually created synsemantic networks,  in which words are linked in relationships to another, vector semantics automatically form word map semantic relations derived from co-occurrences in corpora.\n",
    "\n",
    "Fortunately, there are Python modules that greatly simplify working with word embeddings and also make it straightforward to obtain pre-trained models.\n",
    "In this tutorial, you will work with Gensim, a Python library specifically\n",
    "for working with semantic vectors (https://radimrehurek.com/gensim/index.html).\n",
    "\n",
    "Work through the tasks in a Jupyter notebook.\n",
    "\n",
    "\n",
    "## 1. Importing the modules and data\n",
    "### a) Import NLP modules\n",
    "Import pandas, Numpy, NLTK and RE as in the first tutorial.\n",
    "Install via pip or conda Gensim (in your console). Then import Gensim, KeyedVectors from gensim.models and the gensim.downloader as follows:\n",
    "\n",
    "```python\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "954be384-f0d7-419b-8098-e50980cb60ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb9cf4-83c7-4b50-9858-96921bc196f0",
   "metadata": {},
   "source": [
    "### b) Downloading and setting up models\n",
    "You have heard about different attributes and types of word vector models in the lecture, in this tutorial you will compare different models. Pre-trained models can be obtained in several ways:\n",
    "They can be self-trained via custom corpora, obtained via vectors (for example, from https://github.com/stanfordnlp/GloVe ) and transformed into models in Gensim, or, most straightforwardly, retrieved directly via the pre-trained models natively supported by libraries like Gensim.\n",
    "\n",
    "As a Glove model, we have already prepared for you the \"Glove 6b 100\" model from the source mentioned (https://th-koeln.sciebo.de/f/657059193, password is the name of this course, letters all capital, DI..5) . You only need to load it as a model via the \"KeyedVectors\" class in Gensim:\n",
    "\n",
    "```python\n",
    "glove_6b_100_model = KeyedVectors.load_word2vec_format(\"glove.6B.100d.w2vformat.txt6\", binary=False)\n",
    "```\n",
    "Next, use the following command to get a list of all the models contained in Gensim output a list of all models contained in Gensim:\n",
    "\n",
    "```python\n",
    "print(list(gensim.downloader.info()[\"models\"].keys()))\n",
    "```\n",
    "\n",
    "Load the \"glove-twitter-100\" model via \"gensim.downloader.load()\" as follows:\n",
    "```python\n",
    "glove_twitter100_vectors = gensim.downloader.load(\"glove-twitter-100\")\n",
    "```\n",
    "\n",
    "**The process can take time and uses a lot of system memory, if you have problems, skip downloading the pre-trained model via Gensim!**\n",
    "\n",
    "The last method to get vector models we would like to show you is to create a model over a corpus of texts. For example, proceed as follows:\n",
    "```python\n",
    "corpus = gensim.downloader.load('text8')\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "word2vec = Word2Vec(corpus)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd94678-db3e-429c-ac32-db4e3fd6e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_6b_100_model = KeyedVectors.load_word2vec_format(\"datasets/glove.6B.100d.w2vformat.txt6\", binary=False)\n",
    "glove_6b_50_model = KeyedVectors.load_word2vec_format(\"datasets/glove.6B.50d.w2vformat.txt6\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ed8bbb-c511-4754-a56e-f9c353c189f5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 71290 unique words (28.083071371733357%% of original 253854, drops 182564)', 'datetime': '2022-04-29T10:07:41.376861', 'gensim': '4.1.2', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.4.0-109-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 16718844 word corpus (98.3160275555599%% of original 17005207, drops 286363)', 'datetime': '2022-04-29T10:07:41.468123', 'gensim': '4.1.2', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.4.0-109-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 253854 items\n",
      "sample=0.001 downsamples 38 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12506280.016269669 word corpus (74.8%% of prior 16718844)', 'datetime': '2022-04-29T10:07:41.819332', 'gensim': '4.1.2', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.4.0-109-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 71290 words and 100 dimensions: 92677000 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-04-29T10:07:42.449135', 'gensim': '4.1.2', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.4.0-109-generic-x86_64-with-glibc2.10', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 3 workers on 71290 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-04-29T10:07:42.449759', 'gensim': '4.1.2', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.4.0-109-generic-x86_64-with-glibc2.10', 'event': 'train'}\n",
      "EPOCH 1 - PROGRESS: at 8.94% examples, 1099860 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 14.70% examples, 900216 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 22.28% examples, 914797 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 29.39% examples, 910609 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 38.39% examples, 955284 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 47.68% examples, 989834 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 54.61% examples, 971356 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 63.84% examples, 994565 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 71.31% examples, 987545 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 80.42% examples, 1000795 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 86.07% examples, 972306 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 94.53% examples, 978746 words/s, in_qsize 5, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 1 : training on 17005207 raw words (12507548 effective words) took 12.7s, 983505 effective words/s\n",
      "EPOCH 2 - PROGRESS: at 4.70% examples, 572836 words/s, in_qsize 3, out_qsize 2\n",
      "EPOCH 2 - PROGRESS: at 11.88% examples, 726178 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 20.40% examples, 836537 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 28.75% examples, 889956 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 38.04% examples, 945953 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 47.62% examples, 986800 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 54.67% examples, 971529 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 63.67% examples, 990893 words/s, in_qsize 4, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 72.90% examples, 1009169 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 82.48% examples, 1025945 words/s, in_qsize 4, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 91.95% examples, 1040039 words/s, in_qsize 4, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 2 : training on 17005207 raw words (12506645 effective words) took 12.0s, 1044009 effective words/s\n",
      "EPOCH 3 - PROGRESS: at 8.23% examples, 1016720 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 17.28% examples, 1071574 words/s, in_qsize 4, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 26.69% examples, 1107346 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 33.39% examples, 1041718 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 42.27% examples, 1056361 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 50.38% examples, 1048516 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 55.67% examples, 991746 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 59.08% examples, 921355 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 67.96% examples, 941333 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 72.72% examples, 907215 words/s, in_qsize 4, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 79.37% examples, 898612 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 88.54% examples, 918833 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 94.18% examples, 901739 words/s, in_qsize 5, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 3 : training on 17005207 raw words (12506023 effective words) took 14.0s, 891181 effective words/s\n",
      "EPOCH 4 - PROGRESS: at 6.47% examples, 800078 words/s, in_qsize 5, out_qsize 1\n",
      "EPOCH 4 - PROGRESS: at 14.23% examples, 879780 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 20.52% examples, 846100 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 27.57% examples, 856785 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 35.80% examples, 885468 words/s, in_qsize 3, out_qsize 2\n",
      "EPOCH 4 - PROGRESS: at 44.33% examples, 914627 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 53.91% examples, 955143 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 63.26% examples, 980605 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 72.72% examples, 1002175 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 80.13% examples, 992957 words/s, in_qsize 5, out_qsize 1\n",
      "EPOCH 4 - PROGRESS: at 85.60% examples, 964312 words/s, in_qsize 5, out_qsize 1\n",
      "EPOCH 4 - PROGRESS: at 90.59% examples, 936097 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 96.59% examples, 921309 words/s, in_qsize 5, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 4 : training on 17005207 raw words (12505113 effective words) took 13.5s, 926870 effective words/s\n",
      "EPOCH 5 - PROGRESS: at 8.11% examples, 1003877 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 16.93% examples, 1050348 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 26.22% examples, 1083328 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 34.92% examples, 1086349 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 43.62% examples, 1086463 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 52.32% examples, 1086741 words/s, in_qsize 4, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 57.26% examples, 1019705 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 61.73% examples, 962236 words/s, in_qsize 4, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 67.67% examples, 937646 words/s, in_qsize 4, out_qsize 1\n",
      "EPOCH 5 - PROGRESS: at 74.25% examples, 926590 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 79.66% examples, 901520 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 87.01% examples, 902459 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 92.59% examples, 886589 words/s, in_qsize 5, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 5 : training on 17005207 raw words (12506351 effective words) took 14.0s, 894721 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 85026035 raw words (62531680 effective words) took 66.2s, 944467 effective words/s', 'datetime': '2022-04-29T10:08:48.658673', 'gensim': '4.1.2', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.4.0-109-generic-x86_64-with-glibc2.10', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec(vocab=71290, vector_size=100, alpha=0.025)', 'datetime': '2022-04-29T10:08:48.659090', 'gensim': '4.1.2', 'python': '3.8.3 (default, Jul  2 2020, 16:21:59) \\n[GCC 7.3.0]', 'platform': 'Linux-5.4.0-109-generic-x86_64-with-glibc2.10', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as dl\n",
    "\n",
    "corpus = dl.load('text8')\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "word2vec = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb2a421-455e-4882-b07a-7dcd88251941",
   "metadata": {},
   "source": [
    "we can check vector sizes via \"get_vector()\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df7f815d-2339-42a4-9d85-f96f42c44222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100:  100\n",
      "50:  50\n",
      "w2v:  100\n"
     ]
    }
   ],
   "source": [
    "print('100: ',len(glove_6b_100_model.get_vector('king')))\n",
    "print('50: ',len(glove_6b_50_model.get_vector('king')))\n",
    "print('w2v: ',len(word2vec.wv['king']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e35ffb2-5d36-4bf1-9632-2f74dba5cbd7",
   "metadata": {},
   "source": [
    "## 2. Functions of Vector Semantics\n",
    "You should now have between one and three different vector models loaded.\n",
    "Many vector semantic operations can be applied via Gensim's Word2Vec module API (available HERE)). Don't be confused by the naming, the methods are generally applicable to vector models, not just Word2Vec.\n",
    "### a) Find similar expressions.\n",
    "By similar_by_word() method, determine the 10 most similar terms to the words \"summer\", \"salad\", and \"python\". Discuss in the group: what do you notice? Can you see any differences between the models based on larger corpora (glove.6b and glove twitter100) and the Word2Vec model you trained based on the 32MB corpus? How could the differences differences come about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "868278e8-67dc-4691-a1be-4dd959797a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_summer_100 = glove_6b_100_model.similar_by_word(\"summer\") \n",
    "sim_summer_50 = glove_6b_50_model.similar_by_word(\"summer\") \n",
    "sim_summer_word2vec = word2vec.wv.most_similar('summer', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d53967e5-f1dc-41de-865b-6e81e01f32b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sim_summer_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94e3bb56-8c4d-4800-a180-f9b8937dc7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :\n",
      "100: ('winter', 0.8896950483322144) 50: ('winter', 0.919983446598053) w2v: ('winter', 0.8647171854972839)\n",
      "2 :\n",
      "100: ('spring', 0.8580390214920044) 50: ('spring', 0.8946049809455872) w2v: ('spring', 0.814113438129425)\n",
      "3 :\n",
      "100: ('autumn', 0.7742397785186768) 50: ('autumn', 0.8393581509590149) w2v: ('autumn', 0.8006542325019836)\n",
      "4 :\n",
      "100: ('weekend', 0.7385303378105164) 50: ('beginning', 0.8159069418907166) w2v: ('olympics', 0.7069157361984253)\n",
      "5 :\n",
      "100: ('year', 0.7348463535308838) 50: ('starting', 0.7925456762313843) w2v: ('season', 0.6917258501052856)\n",
      "6 :\n",
      "100: ('days', 0.7250120043754578) 50: ('day', 0.7914853096008301) w2v: ('daytime', 0.6646353602409363)\n",
      "7 :\n",
      "100: ('beginning', 0.7218300104141235) 50: ('weekend', 0.7911542654037476) w2v: ('seasons', 0.6619733572006226)\n",
      "8 :\n",
      "100: ('during', 0.7205086946487427) 50: ('during', 0.7859177589416504) w2v: ('afternoon', 0.6513773798942566)\n",
      "9 :\n",
      "100: ('season', 0.7031364440917969) 50: ('days', 0.7764894366264343) w2v: ('weekend', 0.6472250819206238)\n",
      "10 :\n",
      "100: ('day', 0.701505720615387) 50: ('year', 0.7735162973403931) w2v: ('night', 0.6385502219200134)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(i+1,\":\")\n",
    "    print(\"100:\",sim_summer_100[i],\"50:\", sim_summer_50[i],\"w2v:\", sim_summer_word2vec[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13167ef2-6335-480d-98e7-b0fbfb35df6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :\n",
      "100: ('salads', 0.766255259513855) 50: ('pasta', 0.8427600860595703) w2v: ('soup', 0.7483817338943481)\n",
      "2 :\n",
      "100: ('pasta', 0.760292649269104) 50: ('soup', 0.832454264163971) w2v: ('sweet', 0.73320472240448)\n",
      "3 :\n",
      "100: ('tomato', 0.7298500537872314) 50: ('salads', 0.8248024582862854) w2v: ('spicy', 0.7315133213996887)\n",
      "4 :\n",
      "100: ('vinaigrette', 0.7297168970108032) 50: ('mashed', 0.8081055879592896) w2v: ('pork', 0.725369930267334)\n",
      "5 :\n",
      "100: ('lettuce', 0.7067264318466187) 50: ('potato', 0.793901264667511) w2v: ('chicken', 0.723736047744751)\n",
      "6 :\n",
      "100: ('dessert', 0.697432279586792) 50: ('fried', 0.7893426418304443) w2v: ('grilled', 0.7175354361534119)\n",
      "7 :\n",
      "100: ('sauce', 0.6960574984550476) 50: ('baked', 0.7884256839752197) w2v: ('miso', 0.7116280794143677)\n",
      "8 :\n",
      "100: ('spinach', 0.6923962831497192) 50: ('tomato', 0.7836177349090576) w2v: ('roast', 0.7074493169784546)\n",
      "9 :\n",
      "100: ('cheese', 0.6896312832832336) 50: ('potatoes', 0.7813866138458252) w2v: ('stew', 0.7054186463356018)\n",
      "10 :\n",
      "100: ('pesto', 0.6863588690757751) 50: ('sauce', 0.7808839082717896) w2v: ('cakes', 0.6924633979797363)\n"
     ]
    }
   ],
   "source": [
    "sim_salad_100 = glove_6b_100_model.similar_by_word(\"salad\") \n",
    "sim_salad_50 = glove_6b_50_model.similar_by_word(\"salad\") \n",
    "sim_salad_word2vec = word2vec.wv.most_similar('salad', topn=10)\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(i+1,\":\")\n",
    "    print(\"100:\",sim_salad_100[i],\"50:\", sim_salad_50[i],\"w2v:\", sim_salad_word2vec[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee29390b-d220-4fe2-acf9-0663dce8e4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :\n",
      "100: ('monty', 0.6886237263679504) 50: ('reticulated', 0.6916365623474121) w2v: ('monty', 0.8617887496948242)\n",
      "2 :\n",
      "100: ('php', 0.586538553237915) 50: ('spamalot', 0.6635736227035522) w2v: ('animaniacs', 0.732209324836731)\n",
      "3 :\n",
      "100: ('perl', 0.5784407258033752) 50: ('php', 0.6414496898651123) w2v: ('moby', 0.6848551630973816)\n",
      "4 :\n",
      "100: ('cleese', 0.5446676015853882) 50: ('owl', 0.6301496028900146) w2v: ('gilliam', 0.6839032173156738)\n",
      "5 :\n",
      "100: ('flipper', 0.5112984776496887) 50: ('mouse', 0.6275478601455688) w2v: ('muppet', 0.6801720857620239)\n",
      "6 :\n",
      "100: ('ruby', 0.5066928267478943) 50: ('reticulatus', 0.6274471282958984) w2v: ('blaxploitation', 0.6611126065254211)\n",
      "7 :\n",
      "100: ('spamalot', 0.505638837814331) 50: ('perl', 0.6267575025558472) w2v: ('slayer', 0.6601446866989136)\n",
      "8 :\n",
      "100: ('javascript', 0.5030569434165955) 50: ('monkey', 0.6207211017608643) w2v: ('grail', 0.658697783946991)\n",
      "9 :\n",
      "100: ('reticulated', 0.4983375668525696) 50: ('monty', 0.60793536901474) w2v: ('webcomic', 0.6578059196472168)\n",
      "10 :\n",
      "100: ('monkey', 0.49764129519462585) 50: ('scripting', 0.6041731834411621) w2v: ('parody', 0.6552088856697083)\n"
     ]
    }
   ],
   "source": [
    "sim_python_100 = glove_6b_100_model.similar_by_word(\"python\") \n",
    "sim_python_50 = glove_6b_50_model.similar_by_word(\"python\") \n",
    "sim_python_word2vec = word2vec.wv.most_similar('python', topn=10)\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(i+1,\":\")\n",
    "    print(\"100:\",sim_python_100[i],\"50:\", sim_python_50[i],\"w2v:\", sim_python_word2vec[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438e1d2-1ced-4278-818e-d497ff0c6a45",
   "metadata": {},
   "source": [
    "### b) Analogical Inference/ Relational Similarity.\n",
    "Vector semantic models can be used to infer analogies of the form \"A relates to B as A* relates to ...?\" as they exist in the information extracted from the corpus. Using the method most similar() (see Gensim Word2Vec module API linked in introduction to question 2), you can thus have \"Paris-France+Italy\" find \"Rome\", for example:\n",
    "\n",
    "```python\n",
    "print('glove: Paris - France + Italy: ', glove_6b_100_model.most_similar(positive=[\"paris\", \"italy\"], negative=[\"france\"], topn=3))\n",
    "```\n",
    "\n",
    "Using this method, bias contained in the corpus, more precisely the vector models, can be revealed. For dichotomous features like [man,woman] or [young,old], put one of each in \"positive\" and \"negative\". Find out what the models for \"doctor+woman\", \"housewife+man\" as well as \"car+sea\" contain as their three next associations, choosing meaningful negatives. Discuss as a group! Can you find any other relational symmetries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34a34142-ef07-4312-ab69-1e8e3da392a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove 100: Paris - France + Italy:  [('rome', 0.8189547061920166), ('milan', 0.7376196980476379), ('naples', 0.7117615342140198)]\n"
     ]
    }
   ],
   "source": [
    "print('glove 100: Paris - France + Italy: ', glove_6b_100_model.most_similar(positive=[\"paris\", \"italy\"], negative=[\"france\"], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eaa0e0e4-bd90-4bbd-a609-3960b94b7c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove 50: Paris - France + Italy:  [('rome', 0.8465589284896851), ('milan', 0.7766007781028748), ('turin', 0.7666355967521667)]\n"
     ]
    }
   ],
   "source": [
    "print('glove 50: Paris - France + Italy: ', glove_6b_50_model.most_similar(positive=[\"paris\", \"italy\"], negative=[\"france\"], topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c7fc04-a599-4c8c-85ec-4812158e9506",
   "metadata": {},
   "source": [
    "for word2vec, add \".wv\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4f7c8b4-4412-45db-b997-231f17d37df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2V: Paris - France + Italy:  [('venice', 0.7544799447059631), ('vienna', 0.7467787861824036), ('florence', 0.7297069430351257)]\n"
     ]
    }
   ],
   "source": [
    "print('Word2Vec: Paris - France + Italy: ', word2vec.wv.most_similar(positive=[\"paris\", \"italy\"], negative=[\"france\"], topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55573f33-6030-49ea-bef2-44d6e321b89f",
   "metadata": {},
   "source": [
    "doctor  + woman :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd71b4b6-e55b-4dff-b79d-1d2509a1275f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove 50: Doctor - Man + Woman:  [('nurse', 0.8404642939567566), ('child', 0.7663259506225586), ('pregnant', 0.7570130228996277)]\n"
     ]
    }
   ],
   "source": [
    "print('glove 50: Doctor - Man + Woman: ', glove_6b_50_model.most_similar(positive=[\"doctor\", \"woman\"], negative=[\"man\"], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0ef64d0-8d38-4491-95f7-b782d3155a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec: Doctor - Man + Woman:  [('nurse', 0.6485609412193298), ('teacher', 0.5760821104049683), ('child', 0.5707877278327942)]\n"
     ]
    }
   ],
   "source": [
    "print('Word2Vec: Doctor - Man + Woman: ', word2vec.wv.most_similar(positive=[\"doctor\", \"woman\"], negative=[\"man\"], topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a900f1f-beff-4b5c-b227-ed2b34de0fa7",
   "metadata": {},
   "source": [
    "housewife + man :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20009c9e-c931-492e-89e4-466a985c86d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove 100: Housewife - Woman + Man:  [('homemaker', 0.6182607412338257), ('loner', 0.5810955166816711), ('schoolteacher', 0.5793447494506836)]\n"
     ]
    }
   ],
   "source": [
    "print('glove 100: Housewife - Woman + Man: ', glove_6b_100_model.most_similar(positive=[\"housewife\", \"man\"], negative=[\"woman\"], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6770817c-7e4a-43c4-8afb-42025fbc4a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove 50: Housewife - Woman + Man:  [('loner', 0.7505239844322205), ('schoolteacher', 0.7436637282371521), ('homemaker', 0.7377645373344421)]\n"
     ]
    }
   ],
   "source": [
    "print('glove 50: Housewife - Woman + Man: ', glove_6b_50_model.most_similar(positive=[\"housewife\", \"man\"], negative=[\"woman\"], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ca6c12e0-d780-4b48-b4e5-105425cc53b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec: Housewife - Woman + Man:  [('classmate', 0.6363623142242432), ('youngster', 0.6295291185379028), ('joanie', 0.6116021275520325)]\n"
     ]
    }
   ],
   "source": [
    "print('Word2Vec: Housewife - Woman + Man: ', word2vec.wv.most_similar(positive=[\"housewife\", \"man\"], negative=[\"woman\"], topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9488b4ba-0ee1-4025-937d-fa7200332b66",
   "metadata": {},
   "source": [
    "car + sea :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ee3f06d-065f-4ec3-b72b-9e9900e0be97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove 100: Car - Road + Sea:  [('tanker', 0.6246403455734253), ('ship', 0.6190789341926575), ('jet', 0.5916841626167297)]\n"
     ]
    }
   ],
   "source": [
    "print('glove 100: Car - Road + Sea: ', glove_6b_100_model.most_similar(positive=[\"car\", \"sea\"], negative=[\"road\"], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c209470-0c1e-4e70-a16d-71abe162fcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove 50: Car - Road + Sea:  [('jet', 0.7649568915367126), ('plane', 0.7592712640762329), ('cargo', 0.7386853098869324)]\n"
     ]
    }
   ],
   "source": [
    "print('glove 50: Car - Road + Sea: ', glove_6b_50_model.most_similar(positive=[\"car\", \"sea\"], negative=[\"road\"], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e8c8a3d9-d579-405f-8247-7b199a36a688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec: Car - Road + Sea:  [('carrier', 0.5291203260421753), ('carriers', 0.5250734090805054), ('aircraft', 0.5171101093292236)]\n"
     ]
    }
   ],
   "source": [
    "print('Word2Vec: Car - Road + Sea: ', word2vec.wv.most_similar(positive=[\"car\", \"sea\"], negative=[\"road\"], topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3f78b-3ff1-4be9-ad68-7a1f99e99d94",
   "metadata": {},
   "source": [
    "### c) Bonus: (Sentence) Similarity.\n",
    "The method \"n_similarity([], [])\" can be used to judge the similarity of two lists of tokens to each other. Can you think of a useful application example? Discuss and try!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37432f1d-dd3e-4768-a21b-17f1eee36af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35846308"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_6b_100_model.n_similarity(['game', \"deer\"], [\"veil\", \"cow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "113888c3-a391-4381-8d08-9abadad8f491",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"Separate the yolks from the whites and slowly add them slowly.\"\n",
    "sentence2 = \"Fry the pancakes until they become golden brown.\"\n",
    "sentence3 = \"If you push with your left foot, your stance is called goofy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8643fb59-5dc4-4638-86cc-968a23cef28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6e00f5e0-5f22-4712-9562-5d9d33baedd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 and 2: 0.8566468\n"
     ]
    }
   ],
   "source": [
    "print(\"1 and 2:\",glove_6b_100_model.n_similarity([a.lower() for a in tokenizer.tokenize(sentence1)], [a.lower() for a in tokenizer.tokenize(sentence2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a6c745e2-eb20-4533-b427-09aa3f42b6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 and 3: 0.80041516\n"
     ]
    }
   ],
   "source": [
    "print(\"1 and 3:\",glove_6b_100_model.n_similarity([a.lower() for a in tokenizer.tokenize(sentence1)], [a.lower() for a in tokenizer.tokenize(sentence3)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ab7f4ae8-1433-42b2-b18d-1938d4e00d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 and 3: 0.802222\n"
     ]
    }
   ],
   "source": [
    "print(\"2 and 3:\",glove_6b_100_model.n_similarity([a.lower() for a in tokenizer.tokenize(sentence2)], [a.lower() for a in tokenizer.tokenize(sentence3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b04cda-beed-4438-bf13-86ca18b44fe0",
   "metadata": {},
   "source": [
    "## 3. Pre-trained German vector models and vector-similarity with SpaCy\n",
    "### a) Import modules and data\n",
    "As you have learned in the second tutorial, Spacy is a powerful tool, that often has accessible, powerful functionalities built in, including word vector embeddings. Download a large German model, for example in your jupyter notebook by entering with the following command:\n",
    "```python\n",
    "!python -m spacy download de_core_news_lg\n",
    "```\n",
    "\n",
    "Attention: You will probably have to restart your Jupyter kernel for your system to find the can find the model. Load the model in your notebook with \n",
    "\n",
    "```python\n",
    "nlp = spacy.load('de_core_news_lg')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "14b0dc5d-d812-4085-b543-877e417f8745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 17:17:26.696576: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-28 17:17:26.696645: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "95adcc84-3995-4134-8b55-b8a473bae924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-28 17:17:36.359689: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-28 17:17:36.359722: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting de-core-news-lg==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_lg-3.2.0/de_core_news_lg-3.2.0-py3-none-any.whl (572.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 572.3 MB 20 kB/s s eta 0:00:01    |████▋                           | 81.8 MB 2.4 MB/s eta 0:03:27        | 407.6 MB 24.3 MB/s eta 0:00:07��████████████████████████▌   | 509.7 MB 29.8 MB/s eta 0:00:03     |██████████████████████████████  | 536.5 MB 14.0 MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /home/fabian/.local/lib/python3.8/site-packages (from de-core-news-lg==3.2.0) (3.2.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/fabian/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/fabian/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (1.0.2)\n",
      "Requirement already satisfied: jinja2 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (3.0.1)\n",
      "Requirement already satisfied: setuptools in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/fabian/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/fabian/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/fabian/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (2.4.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/fabian/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/fabian/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/fabian/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/fabian/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/fabian/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/fabian/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (21.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/fabian/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/fabian/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (1.21.2)\n",
      "Requirement already satisfied: click<8.1.0 in /home/fabian/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (8.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/fabian/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/fabian/.local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/fabian/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (2021.5.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c265fafb-37bd-4d2d-a5f2-fab55758abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71c6c35-ce64-4a2b-a426-be00dc975eb5",
   "metadata": {},
   "source": [
    "### b) Similarity with SpaCy\n",
    "\n",
    "Using pandas read_csv(), load the list of single term suggestions.txt from the previous tutorial and save it as a dataframe. Add another column \"city\" in which you determine the similarity of the term (suggestion_ger) to the document \"nlp(\"city\")\". Do the same for \"politics\" and \"recreation\" and store the similarities in meaningful the similarities in meaningful named columns. \n",
    "Find the similarity between two texts using the \".similarity\" method:\n",
    "```python\n",
    "simil = nlp(\"text number one\").similarity(nlp(\"text number two\"))\n",
    "```\n",
    "Discuss:\n",
    "What could you use the particular similarities for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bd49f1b4-4bd5-4a81-b8c9-d492f37dcef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "simil = nlp(\"text number one\").similarity(nlp(\"text number two\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fd67d75b-1527-47fd-aec7-cf9364a254b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 and 2: 0.8281328035011029\n",
      "1 and 3: 0.7779656503070352\n",
      "2 and 3: 0.77883248259888\n"
     ]
    }
   ],
   "source": [
    "print(\"1 and 2:\",nlp(sentence1).similarity(nlp(sentence2)))\n",
    "print(\"1 and 3:\",nlp(sentence1).similarity(nlp(sentence3)))\n",
    "print(\"2 and 3:\",nlp(sentence2).similarity(nlp(sentence3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c0bc4-e788-46a2-98d6-e86ca8515a85",
   "metadata": {},
   "source": [
    "now for the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ef290543-f4dd-465b-991b-6d80f8ea3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d4af1146-b78e-49d9-bcc1-326d03fda83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ger = pd.read_csv(\"single_term_suggestions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f828e8dd-e0a8-4f32-b185-0609f797a61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108825/2336214784.py:1: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  data_ger[\"city\"] = data_ger.apply(lambda row: nlp(row[\"suggestion_ger\"]).similarity(nlp(\"city\")), axis=1)\n"
     ]
    }
   ],
   "source": [
    "data_ger[\"city\"] = data_ger.apply(lambda row: nlp(row[\"suggestion_ger\"]).similarity(nlp(\"city\")), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "18583dac-0dfa-4936-9827-4df11f51b581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108825/3348679742.py:1: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  data_ger[\"politics\"] = data_ger.apply(lambda row: nlp(row[\"suggestion_ger\"]).similarity(nlp(\"politics\")), axis=1)\n"
     ]
    }
   ],
   "source": [
    "data_ger[\"politics\"] = data_ger.apply(lambda row: nlp(row[\"suggestion_ger\"]).similarity(nlp(\"politics\")), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8b7f2599-f17b-431f-bbae-49ee8204a7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108825/313282341.py:1: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  data_ger[\"recreation\"] = data_ger.apply(lambda row: nlp(row[\"suggestion_ger\"]).similarity(nlp(\"recreation\")), axis=1)\n"
     ]
    }
   ],
   "source": [
    "data_ger[\"recreation\"] = data_ger.apply(lambda row: nlp(row[\"suggestion_ger\"]).similarity(nlp(\"recreation\")), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "103e2f24-1f96-4135-a910-20f02a17c550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108825/2448204227.py:1: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  data_ger[\"privat\"] = data_ger.apply(lambda row: nlp(row[\"suggestion_ger\"]).similarity(nlp(\"privat\")), axis=1)\n"
     ]
    }
   ],
   "source": [
    "data_ger[\"privat\"] = data_ger.apply(lambda row: nlp(row[\"suggestion_ger\"]).similarity(nlp(\"privat\")), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "347bd019-7fb8-4965-84f9-a4818fdf853a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suggestion_ger</th>\n",
       "      <th>city</th>\n",
       "      <th>politics</th>\n",
       "      <th>recreation</th>\n",
       "      <th>privat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>0.027505</td>\n",
       "      <td>0.043489</td>\n",
       "      <td>-0.007406</td>\n",
       "      <td>0.128290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aach</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>-0.003878</td>\n",
       "      <td>0.135336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aalten</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aarburg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaronn</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>zwangsdienst</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3758</th>\n",
       "      <td>zwangshypothek</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3759</th>\n",
       "      <td>zweibruecken</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760</th>\n",
       "      <td>zwickau</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>0.079084</td>\n",
       "      <td>0.081898</td>\n",
       "      <td>0.322399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3761</th>\n",
       "      <td>zwillinge</td>\n",
       "      <td>0.304903</td>\n",
       "      <td>0.249109</td>\n",
       "      <td>0.300339</td>\n",
       "      <td>0.299101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3762 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      suggestion_ger      city  politics  recreation    privat\n",
       "0                 aa  0.027505  0.043489   -0.007406  0.128290\n",
       "1               aach  0.141100  0.002952   -0.003878  0.135336\n",
       "2             aalten  0.000000  0.000000    0.000000  0.000000\n",
       "3            aarburg  0.000000  0.000000    0.000000  0.000000\n",
       "4             aaronn  0.000000  0.000000    0.000000  0.000000\n",
       "...              ...       ...       ...         ...       ...\n",
       "3757    zwangsdienst  0.000000  0.000000    0.000000  0.000000\n",
       "3758  zwangshypothek  0.000000  0.000000    0.000000  0.000000\n",
       "3759    zweibruecken  0.000000  0.000000    0.000000  0.000000\n",
       "3760         zwickau  0.312800  0.079084    0.081898  0.322399\n",
       "3761       zwillinge  0.304903  0.249109    0.300339  0.299101\n",
       "\n",
       "[3762 rows x 5 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ger"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
